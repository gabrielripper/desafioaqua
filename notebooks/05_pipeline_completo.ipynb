{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f29ef2",
   "metadata": {},
   "source": [
    "## 1. Configuração e Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf7ddaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports essenciais\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import duckdb\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import xgboost as xgb\n",
    "\n",
    "# Configurações\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d943093",
   "metadata": {},
   "source": [
    "## 2. Classe Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7116e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    \n",
    "    def __init__(self, data_path='../data/', models_path='../models/'):\n",
    "        self.data_path = data_path\n",
    "        self.models_path = models_path\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Criando pasta de modelos se não existir\n",
    "        os.makedirs(self.models_path, exist_ok=True)\n",
    "        \n",
    "        # Containers para dados e modelos\n",
    "        self.raw_data = {}\n",
    "        self.processed_data = {}\n",
    "        self.models = {}\n",
    "        self.encoders = {}\n",
    "        self.feature_columns = []\n",
    "    \n",
    "    def load_raw_data(self):\n",
    "        print(\"Carregando dados CSV\")\n",
    "        \n",
    "        try:\n",
    "            self.raw_data['consumo'] = pd.read_csv(f'{self.data_path}consumo.csv')\n",
    "            self.raw_data['clima'] = pd.read_csv(f'{self.data_path}clima.csv')\n",
    "            self.raw_data['clientes'] = pd.read_csv(f'{self.data_path}clientes.csv')\n",
    "            \n",
    "            # Convertendo datas\n",
    "            self.raw_data['consumo']['date'] = pd.to_datetime(self.raw_data['consumo']['date'])\n",
    "            self.raw_data['clima']['date'] = pd.to_datetime(self.raw_data['clima']['date'])\n",
    "            \n",
    "            print(f\"Dados carregados - Consumo: {len(self.raw_data['consumo']):,}, Clima: {len(self.raw_data['clima']):,}, Clientes: {len(self.raw_data['clientes']):,}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def process_data(self):\n",
    "        print(\"Tratando dados...\")\n",
    "        \n",
    "        # Copiando dados para processamento\n",
    "        clima_tratado = self.raw_data['clima'].copy()\n",
    "        clientes_tratado = self.raw_data['clientes'].copy()\n",
    "        consumo_tratado = self.raw_data['consumo'].copy()\n",
    "        \n",
    "        # Tratamento de valores faltantes de temperatura\n",
    "        print(\"Imputando valores faltantes de temperatura...\")\n",
    "        clima_tratado = self._imputar_temperatura_regional(clima_tratado)\n",
    "        \n",
    "        # Tratamento de clientes com região desconhecida\n",
    "        print(\"Imputando regiões desconhecidas...\")\n",
    "        clientes_tratado = self._imputar_regioes_desconhecidas(clientes_tratado, consumo_tratado)\n",
    "        \n",
    "        # Salvando dados tratados\n",
    "        self.processed_data['clima'] = clima_tratado\n",
    "        self.processed_data['clientes'] = clientes_tratado\n",
    "        self.processed_data['consumo'] = consumo_tratado\n",
    "        \n",
    "        print(\"Dados tratados com sucesso!\")\n",
    "    \n",
    "    def _imputar_temperatura_regional(self, df_clima):\n",
    "        df_temp = df_clima.copy()\n",
    "        valores_faltantes = df_temp['temperature'].isnull().sum()\n",
    "        \n",
    "        if valores_faltantes == 0:\n",
    "            return df_temp\n",
    "        \n",
    "        for idx in df_temp[df_temp['temperature'].isnull()].index:\n",
    "            regiao = df_temp.loc[idx, 'region']\n",
    "            data = df_temp.loc[idx, 'date']\n",
    "            \n",
    "            # Buscar valores de temperatura da mesma região em um período de 7 dias\n",
    "            inicio = data - pd.Timedelta(days=7)\n",
    "            fim = data + pd.Timedelta(days=7)\n",
    "            \n",
    "            valores_proximos = df_temp[\n",
    "                (df_temp['region'] == regiao) & \n",
    "                (df_temp['date'] >= inicio) & \n",
    "                (df_temp['date'] <= fim) & \n",
    "                (df_temp['temperature'].notna())\n",
    "            ]['temperature']\n",
    "            \n",
    "            if len(valores_proximos) > 0:\n",
    "                df_temp.loc[idx, 'temperature'] = valores_proximos.mean()\n",
    "            else:\n",
    "                # Se não houver valores próximos, usar a média geral da região\n",
    "                media_regiao = df_temp[df_temp['region'] == regiao]['temperature'].mean()\n",
    "                df_temp.loc[idx, 'temperature'] = media_regiao\n",
    "        \n",
    "        return df_temp\n",
    "    \n",
    "    def _imputar_regioes_desconhecidas(self, df_clientes, df_consumo):\n",
    "        # Calculando perfil de consumo por cliente\n",
    "        perfil_consumo = df_consumo.groupby('client_id')['consumption_kwh'].agg([\n",
    "            'mean', 'std', 'min', 'max', 'median'\n",
    "        ]).reset_index()\n",
    "        \n",
    "        # Adicionando região aos perfis\n",
    "        perfil_consumo = perfil_consumo.merge(df_clientes, on='client_id', how='left')\n",
    "        \n",
    "        # Separando clientes conhecidos e desconhecidos\n",
    "        perfil_conhecidos = perfil_consumo[perfil_consumo['region'] != 'Desconhecida'].copy()\n",
    "        perfil_desconhecidos = perfil_consumo[perfil_consumo['region'] == 'Desconhecida'].copy()\n",
    "        \n",
    "        if len(perfil_desconhecidos) == 0:\n",
    "            return df_clientes\n",
    "        \n",
    "        # Calculando perfil médio por região conhecida\n",
    "        perfil_por_regiao = perfil_conhecidos.groupby('region')[['mean', 'std', 'min', 'max', 'median']].mean()\n",
    "        \n",
    "        # Imputando região para clientes desconhecidos\n",
    "        df_clientes_tratado = df_clientes.copy()\n",
    "        \n",
    "        for idx, cliente in perfil_desconhecidos.iterrows():\n",
    "            cliente_id = cliente['client_id']\n",
    "            regiao_similar = self._encontrar_regiao_similar(cliente, perfil_por_regiao)\n",
    "            \n",
    "            # Atualizando a região\n",
    "            df_clientes_tratado.loc[df_clientes_tratado['client_id'] == cliente_id, 'region'] = regiao_similar\n",
    "        \n",
    "        return df_clientes_tratado\n",
    "    \n",
    "    def _encontrar_regiao_similar(self, cliente_perfil, perfis_regionais):\n",
    "        features = ['mean', 'std', 'min', 'max', 'median']\n",
    "        \n",
    "        # Preparando os dados para normalização\n",
    "        dados_completos = pd.concat([\n",
    "            perfis_regionais[features],\n",
    "            cliente_perfil[features].to_frame().T\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        # Normalizando os dados\n",
    "        scaler = MinMaxScaler()\n",
    "        dados_normalizados = scaler.fit_transform(dados_completos)\n",
    "        \n",
    "        # Separando dados normalizados\n",
    "        perfis_norm = dados_normalizados[:-1]\n",
    "        cliente_norm = dados_normalizados[-1:]\n",
    "        \n",
    "        # Calculando distâncias\n",
    "        distancias_array = euclidean_distances(cliente_norm, perfis_norm)[0]\n",
    "        \n",
    "        # Criando dicionário de distâncias por região\n",
    "        distancias = dict(zip(perfis_regionais.index, distancias_array))\n",
    "        \n",
    "        # Retornando região com menor distância\n",
    "        return min(distancias, key=distancias.get)\n",
    "\n",
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc82b14",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22e8845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_with_duckdb(pipeline):\n",
    "    print(\"Criando features com DuckDB.\")\n",
    "    \n",
    "    # Conectando ao DuckDB\n",
    "    conn = duckdb.connect(':memory:')\n",
    "    \n",
    "    try:\n",
    "        # Carregando dados processados\n",
    "        consumo_df = pipeline.processed_data['consumo']\n",
    "        clima_df = pipeline.processed_data['clima']\n",
    "        clientes_df = pipeline.processed_data['clientes']\n",
    "        \n",
    "        # Criando tabelas no DuckDB\n",
    "        conn.execute(\"DROP TABLE IF EXISTS consumo\")\n",
    "        conn.execute(\"DROP TABLE IF EXISTS clima\")  \n",
    "        conn.execute(\"DROP TABLE IF EXISTS clientes\")\n",
    "        \n",
    "        # Tabela de Clientes\n",
    "        conn.execute(\"\"\"\n",
    "            CREATE TABLE clientes (\n",
    "                client_id VARCHAR PRIMARY KEY,\n",
    "                region VARCHAR NOT NULL\n",
    "            );\n",
    "        \"\"\")\n",
    "        conn.execute(\"INSERT INTO clientes SELECT * FROM clientes_df\")\n",
    "        \n",
    "        # Tabela de Clima\n",
    "        conn.execute(\"\"\"\n",
    "            CREATE TABLE clima (\n",
    "                region VARCHAR NOT NULL,\n",
    "                date DATE NOT NULL,\n",
    "                temperature DOUBLE NOT NULL,\n",
    "                humidity DOUBLE NOT NULL,\n",
    "                PRIMARY KEY (region, date)\n",
    "            );\n",
    "        \"\"\")\n",
    "        conn.execute(\"INSERT INTO clima SELECT * FROM clima_df\")\n",
    "        \n",
    "        # Tabela de Consumo\n",
    "        conn.execute(\"\"\"\n",
    "            CREATE TABLE consumo (\n",
    "                client_id VARCHAR NOT NULL,\n",
    "                date DATE NOT NULL,\n",
    "                consumption_kwh DOUBLE NOT NULL,\n",
    "                PRIMARY KEY (client_id, date),\n",
    "                FOREIGN KEY (client_id) REFERENCES clientes(client_id)\n",
    "            );\n",
    "        \"\"\")\n",
    "        conn.execute(\"INSERT INTO consumo SELECT * FROM consumo_df\")\n",
    "        \n",
    "        # Criando dataset com features\n",
    "        print(\"Criando features avançadas.\")\n",
    "        \n",
    "        dataset_final = conn.execute(\"\"\"\n",
    "            SELECT \n",
    "                co.client_id,\n",
    "                co.date,\n",
    "                co.consumption_kwh as target,\n",
    "                cl.temperature,\n",
    "                cl.humidity,\n",
    "                -- Calculando sensação térmica (Heat Index)\n",
    "                CASE \n",
    "                    WHEN ((cl.temperature * 9/5) + 32) < 80 THEN cl.temperature\n",
    "                    ELSE ROUND(\n",
    "                        ((-42.379 + \n",
    "                          2.04901523 * ((cl.temperature * 9/5) + 32) + \n",
    "                          10.14333127 * cl.humidity - \n",
    "                          0.22475541 * ((cl.temperature * 9/5) + 32) * cl.humidity - \n",
    "                          6.83783e-3 * POWER(((cl.temperature * 9/5) + 32), 2) - \n",
    "                          5.481717e-2 * POWER(cl.humidity, 2) + \n",
    "                          1.22874e-3 * POWER(((cl.temperature * 9/5) + 32), 2) * cl.humidity + \n",
    "                          8.5282e-4 * ((cl.temperature * 9/5) + 32) * POWER(cl.humidity, 2) - \n",
    "                          1.99e-6 * POWER(((cl.temperature * 9/5) + 32), 2) * POWER(cl.humidity, 2)\n",
    "                        ) - 32) * 5/9, 2)\n",
    "                END as sensacao_termica,\n",
    "                c.region,\n",
    "                EXTRACT(YEAR FROM co.date) as year,\n",
    "                EXTRACT(MONTH FROM co.date) as month,\n",
    "                EXTRACT(DAY FROM co.date) as day,\n",
    "                EXTRACT(DOW FROM co.date) as day_of_week,\n",
    "                EXTRACT(DOY FROM co.date) as day_of_year,\n",
    "                LAG(co.consumption_kwh, 1) OVER (PARTITION BY co.client_id ORDER BY co.date) as consumption_lag1,\n",
    "                LAG(co.consumption_kwh, 7) OVER (PARTITION BY co.client_id ORDER BY co.date) as consumption_lag7,\n",
    "                AVG(co.consumption_kwh) OVER (PARTITION BY co.client_id ORDER BY co.date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as consumption_ma7\n",
    "            FROM consumo co\n",
    "            JOIN clientes c ON co.client_id = c.client_id\n",
    "            JOIN clima cl ON c.region = cl.region AND co.date = cl.date\n",
    "            ORDER BY co.client_id, co.date\n",
    "        \"\"\").fetchdf()\n",
    "        \n",
    "        # Removendo registros sem lag features (início das séries)\n",
    "        dataset_final = dataset_final[dataset_final['consumption_lag1'].notna()].reset_index(drop=True)\n",
    "        \n",
    "        pipeline.processed_data['dataset_final'] = dataset_final\n",
    "        \n",
    "        print(f\"Features criadas - Dataset final: {len(dataset_final):,} registros\")\n",
    "        \n",
    "        # Salvando dataset para uso posterior\n",
    "        dataset_final.to_csv(f'{pipeline.data_path}dataset_modelagem.csv', index=False)\n",
    "        \n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Adicionando método à classe\n",
    "pipeline.create_features = lambda: create_features_with_duckdb(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e4de1",
   "metadata": {},
   "source": [
    "## 4. Treinamento de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a267bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(pipeline):\n",
    "    print(\"Treinando modelos...\")\n",
    "    \n",
    "    # Preparando dados\n",
    "    df = pipeline.processed_data['dataset_final'].copy()\n",
    "    df_sorted = df.sort_values(['client_id', 'date']).reset_index(drop=True)\n",
    "    \n",
    "    # Definindo features - usando apenas as features de lag/média móvel\n",
    "    feature_columns = [\n",
    "        'consumption_lag1', 'consumption_lag7', 'consumption_ma7'\n",
    "    ]\n",
    "    pipeline.feature_columns = feature_columns\n",
    "    \n",
    "    X = df_sorted[feature_columns]\n",
    "    y = df_sorted['target']\n",
    "    \n",
    "    # Divisão temporal dos dados\n",
    "    total_records = len(df_sorted)\n",
    "    train_size = int(0.7 * total_records)\n",
    "    val_size = int(0.15 * total_records)\n",
    "    \n",
    "    X_train = X.iloc[:train_size]\n",
    "    y_train = y.iloc[:train_size]\n",
    "    X_val = X.iloc[train_size:train_size + val_size]\n",
    "    y_val = y.iloc[train_size:train_size + val_size]\n",
    "    X_test = X.iloc[train_size + val_size:]\n",
    "    y_test = y.iloc[train_size + val_size:]\n",
    "    \n",
    "    print(f\"Divisão dos dados - Treino: {len(X_train):,}, Val: {len(X_val):,}, Teste: {len(X_test):,}\")\n",
    "    \n",
    "    # Função para calcular métricas\n",
    "    def calcular_metricas(y_true, y_pred):\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        # MAPE\n",
    "        epsilon = 1e-10\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
    "        \n",
    "        return {'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'R2': r2}\n",
    "    \n",
    "    # TimeSeriesSplit para validação\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    \n",
    "    # Random Forest\n",
    "    print(\"Treinando Random Forest...\")\n",
    "    \n",
    "    rf_param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    \n",
    "    rf_grid = GridSearchCV(\n",
    "        RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        rf_param_grid,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_grid.fit(X_train, y_train)\n",
    "    rf_model = rf_grid.best_estimator_\n",
    "    \n",
    "    # Predições Random Forest\n",
    "    rf_train_pred = rf_model.predict(X_train)\n",
    "    rf_val_pred = rf_model.predict(X_val)\n",
    "    rf_test_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    rf_metrics = {\n",
    "        'train': calcular_metricas(y_train, rf_train_pred),\n",
    "        'val': calcular_metricas(y_val, rf_val_pred),\n",
    "        'test': calcular_metricas(y_test, rf_test_pred)\n",
    "    }\n",
    "    \n",
    "    # XGBoost\n",
    "    print(\"Treinando XGBoost...\")\n",
    "    \n",
    "    xgb_param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 6],\n",
    "        'learning_rate': [0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    xgb_grid = GridSearchCV(\n",
    "        xgb.XGBRegressor(random_state=42, n_jobs=-1, verbosity=0),\n",
    "        xgb_param_grid,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    xgb_grid.fit(X_train, y_train)\n",
    "    xgb_model = xgb_grid.best_estimator_\n",
    "    \n",
    "    # Predições XGBoost\n",
    "    xgb_train_pred = xgb_model.predict(X_train)\n",
    "    xgb_val_pred = xgb_model.predict(X_val)\n",
    "    xgb_test_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    xgb_metrics = {\n",
    "        'train': calcular_metricas(y_train, xgb_train_pred),\n",
    "        'val': calcular_metricas(y_val, xgb_val_pred),\n",
    "        'test': calcular_metricas(y_test, xgb_test_pred)\n",
    "    }\n",
    "    \n",
    "    # Comparando modelos e selecionando o melhor\n",
    "    rf_test_rmse = rf_metrics['test']['RMSE']\n",
    "    xgb_test_rmse = xgb_metrics['test']['RMSE']\n",
    "    \n",
    "    if rf_test_rmse < xgb_test_rmse:\n",
    "        best_model = rf_model\n",
    "        best_model_name = 'Random Forest'\n",
    "        best_metrics = rf_metrics\n",
    "        best_predictions = rf_test_pred\n",
    "    else:\n",
    "        best_model = xgb_model\n",
    "        best_model_name = 'XGBoost'\n",
    "        best_metrics = xgb_metrics\n",
    "        best_predictions = xgb_test_pred\n",
    "    \n",
    "    # Salvando modelos\n",
    "    pipeline.models = {\n",
    "        'best_model': best_model,\n",
    "        'best_model_name': best_model_name,\n",
    "        'random_forest': rf_model,\n",
    "        'xgboost': xgb_model,\n",
    "        'metrics': {\n",
    "            'random_forest': rf_metrics,\n",
    "            'xgboost': xgb_metrics,\n",
    "            'best': best_metrics\n",
    "        },\n",
    "        'test_data': {\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'predictions': best_predictions\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"Melhor modelo: {best_model_name} (RMSE: {best_metrics['test']['RMSE']:.4f})\")\n",
    "    \n",
    "    # Exibindo métricas\n",
    "    print(f\"Random Forest - Teste RMSE: {rf_metrics['test']['RMSE']:.4f}\")\n",
    "    print(f\"XGBoost - Teste RMSE: {xgb_metrics['test']['RMSE']:.4f}\")\n",
    "    print(f\"Melhor modelo: {best_model_name}\")\n",
    "    print(f\"RMSE: {best_metrics['test']['RMSE']:.4f}\")\n",
    "    print(f\"MAE: {best_metrics['test']['MAE']:.4f}\")\n",
    "    print(f\"MAPE: {best_metrics['test']['MAPE']:.2f}%\")\n",
    "    print(f\"R²: {best_metrics['test']['R2']:.4f}\")\n",
    "\n",
    "# Adicionando método à classe\n",
    "pipeline.train_models = lambda: train_models(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578a8f87",
   "metadata": {},
   "source": [
    "## 5. Versionamento e Registro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "619e8dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models_and_metadata(pipeline):\n",
    "    print(\"Salvando modelo...\")\n",
    "    \n",
    "    timestamp = pipeline.timestamp\n",
    "    \n",
    "    # Salvando pipeline\n",
    "    complete_pipeline = {\n",
    "        'model': pipeline.models['best_model'],\n",
    "        'feature_columns': pipeline.feature_columns,\n",
    "        'model_name': pipeline.models['best_model_name'],\n",
    "        'metrics': pipeline.models['metrics']['best'],\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    \n",
    "    pipeline_path = f'{pipeline.models_path}pipeline_{timestamp}.pkl'\n",
    "    with open(pipeline_path, 'wb') as f:\n",
    "        pickle.dump(complete_pipeline, f)\n",
    "    \n",
    "    print(f\"Modelo salvo: pipeline_{timestamp}.pkl\")\n",
    "    \n",
    "    return complete_pipeline\n",
    "\n",
    "# Adicionando método à classe\n",
    "pipeline.save_models = lambda: save_models_and_metadata(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c8ae2",
   "metadata": {},
   "source": [
    "## 6. Função de Inferência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b240a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_system(pipeline):\n",
    "    \n",
    "    def predict_consumption(client_id, date, consumption_lag1, consumption_lag7, consumption_ma7):    \n",
    "        \n",
    "        # Convertendo data se necessário\n",
    "        if isinstance(date, str):\n",
    "            date = pd.to_datetime(date)\n",
    "        \n",
    "        # Montando array de features - apenas as 3 features de lag/média móvel\n",
    "        features = np.array([[\n",
    "            consumption_lag1, consumption_lag7, consumption_ma7\n",
    "        ]])\n",
    "        \n",
    "        # Fazendo predição\n",
    "        prediction = pipeline.models['best_model'].predict(features)[0]\n",
    "        \n",
    "        return {\n",
    "            'client_id': client_id,\n",
    "            'date': date.strftime('%Y-%m-%d'),\n",
    "            'predicted_consumption': round(prediction, 2),\n",
    "            'model_used': pipeline.models['best_model_name'],\n",
    "            'input_features': {\n",
    "                'consumption_lag1': consumption_lag1,\n",
    "                'consumption_lag7': consumption_lag7,\n",
    "                'consumption_ma7': consumption_ma7\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    pipeline.predict = predict_consumption\n",
    "    print(\"Sistema de inferência configurado!\")\n",
    "\n",
    "# Adicionando método à classe\n",
    "pipeline.create_inference = lambda: create_inference_system(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5290508",
   "metadata": {},
   "source": [
    "## 7. Execução do Pipeline Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d39e893a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Pipeline\n",
      "Carregando dados CSV\n",
      "Dados carregados - Consumo: 18,000, Clima: 900, Clientes: 100\n",
      "Tratando dados...\n",
      "Imputando valores faltantes de temperatura...\n",
      "Imputando regiões desconhecidas...\n",
      "Dados tratados com sucesso!\n",
      "Criando features com DuckDB.\n",
      "Criando features avançadas.\n",
      "Features criadas - Dataset final: 17,900 registros\n",
      "Treinando modelos...\n",
      "Divisão dos dados - Treino: 12,530, Val: 2,685, Teste: 2,685\n",
      "Treinando Random Forest...\n",
      "Treinando XGBoost...\n",
      "Melhor modelo: XGBoost (RMSE: 2.0213)\n",
      "Random Forest - Teste RMSE: 2.0420\n",
      "XGBoost - Teste RMSE: 2.0213\n",
      "Melhor modelo: XGBoost\n",
      "RMSE: 2.0213\n",
      "MAE: 1.6066\n",
      "MAPE: 11.43%\n",
      "R²: 0.7096\n",
      "Salvando modelo...\n",
      "Modelo salvo: pipeline_20250813_204748.pkl\n",
      "Sistema de inferência configurado!\n",
      "ID da execução: 20250813_204748\n",
      "Melhor modelo: XGBoost\n",
      "RMSE no teste: 2.0213\n"
     ]
    }
   ],
   "source": [
    "# Executando o pipeline completo\n",
    "print(\"Iniciando Pipeline\")\n",
    "\n",
    "try:\n",
    "    # Etapa 1: Ingestão de dados\n",
    "    pipeline.load_raw_data()\n",
    "    \n",
    "    # Etapa 2: Tratamento de dados\n",
    "    pipeline.process_data()\n",
    "    \n",
    "    # Etapa 3: Feature engineering\n",
    "    pipeline.create_features()\n",
    "    \n",
    "    # Etapa 4: Treinamento de modelos\n",
    "    pipeline.train_models()\n",
    "    \n",
    "    # Etapa 5: Versionamento e salvamento\n",
    "    saved_pipeline = pipeline.save_models()\n",
    "    \n",
    "    # Etapa 6: Sistema de inferência\n",
    "    pipeline.create_inference()\n",
    "        \n",
    "    print(f\"ID da execução: {pipeline.timestamp}\")\n",
    "    print(f\"Melhor modelo: {pipeline.models['best_model_name']}\")\n",
    "    print(f\"RMSE no teste: {pipeline.models['metrics']['best']['test']['RMSE']:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a2754",
   "metadata": {},
   "source": [
    "## 8. Inferência direta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc98aaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplo 1\n",
      "Cliente: CLIENT_001\n",
      "Data: 2024-01-15\n",
      "Consumo previsto: 21.059999465942383 kWh\n",
      "Features usadas: {'consumption_lag1': 23.5, 'consumption_lag7': 21.2, 'consumption_ma7': 22.8}\n",
      "Modelo usado: XGBoost\n",
      "\n",
      "Exemplo 2\n",
      "Cliente: CLIENT_002\n",
      "Data: 2024-06-20\n",
      "Consumo previsto: 14.550000190734863 kWh\n",
      "Features usadas: {'consumption_lag1': 15.3, 'consumption_lag7': 16.7, 'consumption_ma7': 14.5}\n",
      "Modelo usado: XGBoost\n",
      "\n",
      "Exemplo 3\n",
      "Cliente: CLIENT_003\n",
      "Data: 2024-08-10\n",
      "Consumo previsto: 20.93000030517578 kWh\n",
      "Features usadas: {'consumption_lag1': 28.0, 'consumption_lag7': 26.5, 'consumption_ma7': 27.2}\n",
      "Modelo usado: XGBoost\n"
     ]
    }
   ],
   "source": [
    "# Testando inferência\n",
    "# Exemplo 1: Previsão com dados históricos\n",
    "resultado1 = pipeline.predict(\n",
    "    client_id='CLIENT_001',\n",
    "    date='2024-01-15',\n",
    "    consumption_lag1=23.5,\n",
    "    consumption_lag7=21.2,\n",
    "    consumption_ma7=22.8\n",
    ")\n",
    "\n",
    "print(\"Exemplo 1\")\n",
    "print(f\"Cliente: {resultado1['client_id']}\")\n",
    "print(f\"Data: {resultado1['date']}\")\n",
    "print(f\"Consumo previsto: {resultado1['predicted_consumption']} kWh\")\n",
    "print(f\"Features usadas: {resultado1['input_features']}\")\n",
    "print(f\"Modelo usado: {resultado1['model_used']}\")\n",
    "print()\n",
    "\n",
    "# Exemplo 2: Previsão com outro padrão de consumo\n",
    "resultado2 = pipeline.predict(\n",
    "    client_id='CLIENT_002',\n",
    "    date='2024-06-20',\n",
    "    consumption_lag1=15.3,\n",
    "    consumption_lag7=16.7,\n",
    "    consumption_ma7=14.5\n",
    ")\n",
    "\n",
    "print(\"Exemplo 2\")\n",
    "print(f\"Cliente: {resultado2['client_id']}\")\n",
    "print(f\"Data: {resultado2['date']}\")\n",
    "print(f\"Consumo previsto: {resultado2['predicted_consumption']} kWh\")\n",
    "print(f\"Features usadas: {resultado2['input_features']}\")\n",
    "print(f\"Modelo usado: {resultado2['model_used']}\")\n",
    "print()\n",
    "\n",
    "# Exemplo 3: Previsão com padrão de alto consumo\n",
    "resultado3 = pipeline.predict(\n",
    "    client_id='CLIENT_003',\n",
    "    date='2024-08-10',\n",
    "    consumption_lag1=28.0,\n",
    "    consumption_lag7=26.5,\n",
    "    consumption_ma7=27.2\n",
    ")\n",
    "\n",
    "print(\"Exemplo 3\")\n",
    "print(f\"Cliente: {resultado3['client_id']}\")\n",
    "print(f\"Data: {resultado3['date']}\")\n",
    "print(f\"Consumo previsto: {resultado3['predicted_consumption']} kWh\")\n",
    "print(f\"Features usadas: {resultado3['input_features']}\")\n",
    "print(f\"Modelo usado: {resultado3['model_used']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
